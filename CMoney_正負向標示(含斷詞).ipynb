{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21a23de",
   "metadata": {},
   "source": [
    "### CMoney_正負向標示(含斷詞)\n",
    "Version：20220713  \n",
    "Notes：  \n",
    " > 1. 新增正負向標示(根據正負向詞字典)\n",
    " > 2. 修正詞頻重複寫入bug\n",
    " > 3. 尚未使用到WordCloud，暫時不放入程式中\n",
    "* * *\n",
    "**下方首行可做參數修改**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd639a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Article_FileName = \"社群文章0322\" #文章檔案名稱\n",
    "CkipResults_FileName = \"社群文章0322\" #儲存斷詞檔案名稱\n",
    "CkipDict = \"ckip_dict\" #Ckiptagger字典檔案名稱\n",
    "PositiveDict = \"positive_dict\" #正向詞字典檔案名稱\n",
    "NegativeDict = \"negative_dict\" #負向詞字典檔案名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b223e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "import ckiptagger.model_pos\n",
    "import json\n",
    "import jieba\n",
    "import re\n",
    "import csv\n",
    "from csv import reader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "dict_path = \"./dict/\" #字典路徑\n",
    "model_path = \"./model/\" #Ckiptagger model路徑\n",
    "\n",
    "#================================Ckiptagger================================\n",
    "\n",
    "def Ckip_Cut(articleList):\n",
    "    #詞性列表 http://ckipsvr.iis.sinica.edu.tw/papers/category_list.pdf \n",
    "    #pos_list = ['Da', 'Dfa', 'Dfb', 'Di', 'Na', 'Nb', 'Nc', 'Ncd', 'Nd', 'Nv', 'A', 'VA', 'VAC', 'VB', 'VC', 'VD', 'VE', 'VF', 'VH', 'VHC', 'VI', 'VK'] #原程式詞性篩選\n",
    "    #pos_list = ['Na', 'Nb', 'VA', 'VC', 'VLC', 'VD', 'VH', 'VHC'] #16分詞性\n",
    "    #pos_list = ['A', 'Caa', 'Cab', 'Cba', 'Cbb', 'D', 'Da', 'Dfa', 'Dfb', 'Di', 'Dk' , 'FW', 'Na', 'Nb', 'Nc', 'Ncd', 'Nd', 'Nep', 'Neqa', 'Neqb', 'Nes', 'Neu', 'Nf', 'Ng', 'Nh', 'Nv', 'P', 'VA', 'VAC', 'VB', 'VC', 'VCL', 'VD', 'VE', 'VF', 'VG', 'VH', 'VHC', 'VI', 'VJ', 'VK', 'VL'] #基礎詞性篩選(標點符號、有、是、的、語助詞T、感嘆詞I)  \n",
    "    pos_list = ['A', 'Da', 'Dfa', 'Dfb', 'FW', 'Na', 'Nb', 'Nc', 'Ncd', 'Nd', 'Nv', 'VA', 'VAC', 'VB', 'VC', 'VCL', 'VD', 'VE', 'VF', 'VH', 'VHC', 'VI', 'VJ', 'VK', 'VL'] #20220603篩選\n",
    "    \n",
    "    positive_csv = pd.read_csv(dict_path + PositiveDict + \".csv\") #讀取正向詞字典\n",
    "    negative_csv = pd.read_csv(dict_path + NegativeDict + \".csv\") #讀取負向詞字典\n",
    "    positive_list = positive_csv[\"正向詞\"].to_list()\n",
    "    negative_list = negative_csv[\"負向詞\"].to_list()\n",
    "    \n",
    "    with open(dict_path + CkipDict +\".json\", \"r\", encoding = 'utf-8') as d: #讀取字典，字典可自定義\n",
    "        ckip_dict = json.load(d)\n",
    "        \n",
    "    dictionary = ckiptagger.construct_dictionary(ckip_dict) #設定字典格式為指定格式\n",
    "    \n",
    "    #Load model with GPU\n",
    "    ws = WS(model_path, disable_cuda = False) #WS(斷詞)\n",
    "    pos = POS(model_path, disable_cuda = False) #POS(詞性標註)\n",
    "    ner = NER(model_path, disable_cuda = False) #NER(實體辨識)\n",
    "    \n",
    "    #處理斷詞\n",
    "    ws_results = ws(articleList, coerce_dictionary = dictionary)\n",
    "    \n",
    "    #處理詞性標註\n",
    "    pos_results = pos(ws_results)\n",
    "    \n",
    "    with open(\"./\" + CkipResults_FileName + \"_ckip_results.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"詞性\", \"斷詞\"]) #斷詞結果先寫入第一列標題 \n",
    "    ckip_results = [] #儲存斷詞結果\n",
    "    for i in range(len(ws_results)):\n",
    "        remainderWords = [] #僅儲存要的斷詞結果\n",
    "        for j in range(len(ws_results[i])):\n",
    "            if pos_results[i][j] in pos_list: #僅擷取有在詞性列表中的詞\n",
    "                with open(\"./\" + CkipResults_FileName + \"_ckip_results.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "                    csv_write = csv.writer(f)\n",
    "                    csv_write.writerow([pos_results[i][j], ws_results[i][j]]) #寫入斷詞及詞性資料\n",
    "                remainderWords.append(ws_results[i][j])\n",
    "        ckip_results.append(remainderWords)\n",
    "    \n",
    "    #處理正負向標示\n",
    "    with open(\"./\" + CkipResults_FileName + \"_positive_results.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"正向詞\"]) #先寫入第一列標題\n",
    "    positive_results = [] #儲存正向詞結果\n",
    "    for i in range(len(ws_results)):\n",
    "        positiveWords = [] #僅儲存要的正向詞結果\n",
    "        for j in range(len(ws_results[i])):\n",
    "            if ws_results[i][j] in positive_list: #僅擷取有在正向詞字典中的詞\n",
    "                with open(\"./\" + CkipResults_FileName + \"_positive_results.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "                    csv_write = csv.writer(f)\n",
    "                    csv_write.writerow([ws_results[i][j]]) #寫入正向詞\n",
    "                positiveWords.append(ws_results[i][j])\n",
    "        positive_results.append(positiveWords)\n",
    "    \n",
    "    with open(\"./\" + CkipResults_FileName + \"_negative_results.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"負向詞\"]) #先寫入第一列標題\n",
    "    negative_results = [] #儲存負向詞結果\n",
    "    for i in range(len(ws_results)):\n",
    "        negativeWords = [] #僅儲存要的負向詞結果\n",
    "        for j in range(len(ws_results[i])):\n",
    "            if ws_results[i][j] in negative_list: #僅擷取有在負向詞字典中的詞\n",
    "                with open(\"./\" + CkipResults_FileName + \"_negative_results.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "                    csv_write = csv.writer(f)\n",
    "                    csv_write.writerow([ws_results[i][j]]) #寫入負向詞\n",
    "                negativeWords.append(ws_results[i][j])\n",
    "        negative_results.append(negativeWords)\n",
    "    \n",
    "    #處理實體辨識\n",
    "    ner_results = ner(ws_results, pos_results)\n",
    "    \n",
    "    with open(\"./\" + CkipResults_FileName + \"_ner_results.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"類別\", \"實體辨識\"]) #寫入實體辨識第一列標題\n",
    "    df_articles_data = pd.read_csv(\"./\" + Article_FileName + \".csv\")\n",
    "    df_articles_data.dropna(subset = ['文章內容'], inplace = True)\n",
    "    for n in range(len(df_articles_data)):\n",
    "        list_ner_results = list(ner_results.pop()) #抓取最後一筆文章的實體辨識\n",
    "        for i in range(len(list_ner_results)):\n",
    "            with open(\"./\" + CkipResults_FileName + \"_ner_results.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "                csv_write = csv.writer(f)\n",
    "                csv_write.writerow([list_ner_results[i][2], list_ner_results[i][3]]) #寫入類別及實體辨識資料\n",
    "    \n",
    "    del ws \n",
    "    del pos\n",
    "    del ner #釋放內存\n",
    "    \n",
    "    return ckip_results\n",
    "\n",
    "#================================Jieba================================\n",
    "\n",
    "def Jieba_Cut(articleList):\n",
    "    stopWords = [] #儲存停用詞\n",
    "    remainderWords = [] #儲存剩餘要的詞\n",
    "\n",
    "    with open('dict/stopWords.txt', 'r', encoding = 'UTF-8-sig') as f: #取停用詞\n",
    "        for data in f.readlines():\n",
    "            data = data.strip()\n",
    "            stopWords.append(data)\n",
    "    \n",
    "    jieba.load_userdict('dict/jb_dict.txt') #取字典\n",
    "\n",
    "    jieba_results = [] ##儲存斷詞結果\n",
    "    for article in articleList:\n",
    "        try:\n",
    "            atc = re.sub(r'\\W|\\s|\\d|[a-zA-z]', '', article) #刪除數字、英文等\n",
    "            atc = re.sub(r'[^\\u4e00-\\u9fa5]', '', article) #只留中文\n",
    "        except:\n",
    "            input('Error\\n' + str(article))\n",
    "        \n",
    "        words = jieba.cut(atc, cut_all = False) #處理斷詞\n",
    "        remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n' and a != ' ', words)) #篩選掉停用詞、空白、空行\n",
    "        jieba_results.append(remainderWords)\n",
    "        \n",
    "    return jieba_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131485d",
   "metadata": {},
   "source": [
    "### 讀資料並刪除空白文章內容列(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1d40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_articles_data = pd.read_csv(\"./\" + Article_FileName + \".csv\")\n",
    "df_articles_data.dropna(subset = ['文章內容'], inplace = True)\n",
    "df_articles_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec47cc",
   "metadata": {},
   "source": [
    "### 資料整理為list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = df_articles_data['文章內容'].to_list()\n",
    "articles_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36fcd7",
   "metadata": {},
   "source": [
    "### Jieba(測試用)\n",
    "因Jieba斷詞速度較快，做為資料斷詞測試用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf792e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time() #計算程式執行時間用\n",
    "jieba_results = Jieba_Cut(articles_data)\n",
    "end = time.time() #計算程式執行時間用\n",
    "\n",
    "print(\"斷詞執行時間共：\", end - start, \"秒\") #計算程式執行時間用\n",
    "print(\"不精準的預估Ckiptagger所需執行時間最多約為：\", (end - start) * 1100, \"秒\")\n",
    "\n",
    "jieba_results[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfc559",
   "metadata": {},
   "source": [
    "### CkipTagger\n",
    "因效率較差，處理速度很慢是正常的...  \n",
    "可參考上行預估Ckiptagger所需執行時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38ea16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time() #計算程式執行時間用\n",
    "ckip_results = Ckip_Cut(articles_data)\n",
    "end = time.time() #計算程式執行時間用\n",
    "\n",
    "df_ckip_results = pd.read_csv(\"./\" + CkipResults_FileName + \"_ckip_results.csv\")\n",
    "\n",
    "print(\"斷詞的資料筆數共有\", len(df_ckip_results), \"筆\")\n",
    "print(\"斷詞執行時間共：\", end - start, \"秒\") #計算程式執行時間用\n",
    "print(\"斷詞檔案名稱：\" + CkipResults_FileName + \"_ckip_results.csv\")\n",
    "print(\"實體辨識檔案名稱：\" + CkipResults_FileName + \"_ner_results.csv\")\n",
    "print(\"正向詞標示檔案名稱：\" + CkipResults_FileName + \"_positive_results.csv\")\n",
    "print(\"負向詞標示檔案名稱：\" + CkipResults_FileName + \"_negative_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0684d",
   "metadata": {},
   "source": [
    "### 詞頻(斷詞、正負向詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43494d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "temp = []\n",
    "for i in ckip_results:\n",
    "    temp += i\n",
    "wordcount = Counter(temp)\n",
    "\n",
    "with open(\"./\" + CkipResults_FileName + \"_wordcount.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"斷詞詞頻\", \"次數\"]) #先寫入第一列標題\n",
    "for i in range (len(wordcount.most_common())):\n",
    "    with open(\"./\" + CkipResults_FileName + \"_wordcount.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow(wordcount.most_common()[i])\n",
    "\n",
    "wordcount.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "temp = []\n",
    "\n",
    "positive_csv = pd.read_csv(\"./\" + CkipResults_FileName + \"_positive_results.csv\") #讀取正向詞結果\n",
    "positive_list = positive_csv[\"正向詞\"].to_list()\n",
    "\n",
    "for i in positive_list:\n",
    "    temp += [i]\n",
    "wordcount = Counter(temp)\n",
    "\n",
    "with open(\"./\" + CkipResults_FileName + \"_positive_wordcount.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"正向詞詞頻\", \"次數\"]) #先寫入第一列標題\n",
    "for i in range (len(wordcount.most_common())):\n",
    "    with open(\"./\" + CkipResults_FileName + \"_positive_wordcount.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow(wordcount.most_common()[i])\n",
    "\n",
    "wordcount.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62990b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "temp = []\n",
    "\n",
    "negative_csv = pd.read_csv(\"./\" + CkipResults_FileName + \"_negative_results.csv\") #讀取負向詞結果\n",
    "negative_list = negative_csv[\"負向詞\"].to_list()\n",
    "\n",
    "for i in negative_list:\n",
    "    temp += [i]\n",
    "wordcount = Counter(temp)\n",
    "\n",
    "with open(\"./\" + CkipResults_FileName + \"_negative_wordcount.csv\",'w' , newline = '', encoding = 'utf_8_sig') as f: \n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow([\"負向詞詞頻\", \"次數\"]) #先寫入第一列標題\n",
    "for i in range (len(wordcount.most_common())):\n",
    "    with open(\"./\" + CkipResults_FileName + \"_negative_wordcount.csv\",'a' , newline = '', encoding = 'utf_8_sig') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow(wordcount.most_common()[i])\n",
    "\n",
    "wordcount.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
